---
title: "Midterm Report"
author: "SR"
date: "20/10/2019"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
---

# Preparation

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

R Markdown
!!! Set Working Directory: setwd("/Users/dario.siegen/Downloads") !!!
Packages used for this analysis:

```{r libraries, message = FALSE}
library(tidyverse)
library(psych)
library(ggplot2)
library(devtools)
library(readtext)
library(quanteda)
library(quanteda.corpora)
library(tm)
library(SnowballC)
library(dplyr)
library(tidyr)
library(REdaS)
library(caret)
```

# Data Import

Import of training set of documents (300 single media articles as ".txt" files) from working directory
 
```{r data import}

textx <- readtext("training set/*",
                  docvarsfrom = "filenames",
                  dvsep="-")
```

Because every .txt file contained the document variables in rows and not in columns or nodes (as opposed to a regularly structured document), we pre-processed the data to enable the proper labelling and subsequent treatment thereof. The warning messages point to the fact that the data still requires manual attention.

```{r data prep, echo = FALSE}
txtx_split <- str_split(textx$text, "\n", simplify = TRUE)
txtx_split_merged <- as.data.frame(txtx_split) %>% unite(text, 
                                                         c(V10:ncol(txtx_split)), 
                                                         sep = " ", remove = FALSE)
txtx_split_merged[11:ncol(txtx_split_merged)] <- list(NULL)
colnames(txtx_split_merged) <- c("title", "author", "length", "date", "time", 
                                 "medium", "source", "language", "source2", "text")
txtx_split_merged2 <- separate(txtx_split_merged, c(date), c("day","month","year"),
                               sep = " ", remove = TRUE)
txtx_split_merged2 <- separate(txtx_split_merged2, c(length),
                               c("wordcount","words"),
                               sep = " ", remove = TRUE)
txtx_split_merged2$words <- NULL
txtx_split_merged2 <- unite(txtx_split_merged2, "id", c(year, medium, title),
                            sep = "_", remove = FALSE)
```

# Data Treatment

Amid the low quality and low degree of harmonisation of the data, we proceeded to further manually process the data to have a usable dfm. As we will proceed with the analysis and get more familiar with the peculiarities of the data during that process, this list will still be extended. Some selected terms are not eliminated entirely, but substituted with a space. This allows for distance metrics to applied later in the analysis, if needed.

```{r clean data}
txtx_split_merged3 <- txtx_split_merged2

### remove noise

txtx_split_merged3$text <- gsub("(\\[.*?\\])", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("Document.*", "",
                                txtx_split_merged3$text, ignore.case = F)
txtx_split_merged3$text <- gsub("www.telegraph.co.uk#upd", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("ref_src", "", txtx_split_merged3$text,
                                ignore.case = T)
txtx_split_merged3$text <- gsub("www.telegraph.co.uk", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("\\d", " ", txtx_split_merged3$text,
                                ignore.case = T)
txtx_split_merged3$text <- gsub("Times Newspapers Limited", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("Daily Mail Associated Newspapers Limited", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("Copyright", "", txtx_split_merged3$text,
                                ignore.case = T)
txtx_split_merged3$text <- gsub("All rights reserved", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("The Guardian Newspaper Limited", " ",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("The Guardian Newspaper", " ",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("The Guardian", " ",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("Independent Print Ltd", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("pic.twitter.com", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("twitter.com", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("https", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("http", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("etfw", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("t.co", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("twsrc", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("src=", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("BST", "",
                                txtx_split_merged3$text, ignore.case = F)
txtx_split_merged3$text <- gsub("html", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub(" pm ", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("hashtag", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("hash", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("block-time", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("updated-timeUpdated", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("published-time", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("co.uk", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("status", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("Guardian Newspapers limited", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("News United Kingdom Ireland limited", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("News Group Newspapers", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("Independent Digital News and Media Ltd.", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("The Daily Telegraph", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("Telegraph Media Group Ltd", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("The Sunday Telegraph", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("News UK Ireland Limited", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("News United Kingdom & Ireland Limited", "",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("News UK & Ireland Limited", "",
                                txtx_split_merged3$text, ignore.case = T)

### recover Abbreviations

txtx_split_merged3$text <- gsub("\\bUK\\b", "United Kingdom",
                                txtx_split_merged3$text, ignore.case = F)
txtx_split_merged3$text <- gsub("\\bUS\\b", "United States",
                                txtx_split_merged3$text, ignore.case = F)
txtx_split_merged3$text <- gsub("\\bUN\\b", "United Nations",
                                txtx_split_merged3$text, ignore.case = F)
txtx_split_merged3$text <- gsub("\\bEU\\b", "European Union",
                                txtx_split_merged3$text, ignore.case = F)
txtx_split_merged3$text <- gsub("\\bBP\\b", "British Petroleum",
                                txtx_split_merged3$text, ignore.case = F)
txtx_split_merged3$text <- gsub("\\bBPs\\b", "British Petroleum",
                                txtx_split_merged3$text, ignore.case = F)

### remove months (except May because of Theresa May)

txtx_split_merged3$text <- gsub("January", " ",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("February", " ",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("March", " ",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("April", " ",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("June", " ",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("July", " ",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("August", " ",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("September", " ",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("October", " ",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("November", " ",
                                txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("December", " ",
                                txtx_split_merged3$text, ignore.case = T)
```

```{r csv check, write csv to check, message = FALSE}
write.csv(txtx_split_merged3$text, file = "txtx_split_merged_text.csv")
```

# Corpus creation

Next, we created a data corpus:

```{r corpus}
txt_corpus <- corpus(txtx_split_merged3, docid_field = "id", text_field = "text")
txt_corpus_dfm <- dfm(txt_corpus)

```

# Trining Set

At this point we split the data corpus into a training- and a test-set:

```{r training set splitting}
docvars(txt_corpus, "id_numeric") <- 1:ndoc(txt_corpus)

id_train <- sample(1:ndoc(txt_corpus), size = ndoc(txt_corpus)*(4/5), replace=FALSE)

docvars(txt_corpus, "id_numeric") <- 1:ndoc(txt_corpus)

corpus_df_training <- corpus_subset(txt_corpus, id_numeric %in% id_train) %>%
    dfm(stem = FALSE)

corpus_df_test <- corpus_subset(txt_corpus, !id_numeric %in% id_train) %>%
    dfm(stem = FALSE)

corpus_training <- corpus_subset(txt_corpus, id_numeric %in% id_train)
corpus_test <- corpus_subset(txt_corpus, !id_numeric %in% id_train)

```

By continuing with the data set "corpus_training", we only worked with 80% of the data, the training set.

# Tokeninizing

In the next step, we created token vars that we would use for the first part of our analysis, the principal component analysis (PCA). In that process, in line with Greussing and Boomgaarden (2017), we removed numbers, punctuation, hyphens and stopwords. Furthermore, we lower-cased all letters.

```{r tokens}
toks <- tokens(corpus_training, what="word", remove_numbers=TRUE,
               remove_punct=TRUE, remove_hyphens=TRUE,
               include_docvars = TRUE)
toks <- tokens(toks) %>% 
    tokens_remove(stopwords('en'), padding = TRUE)

toks <- tokens(toks) %>% tokens_select(min_nchar = 2)
```

# Collocations

By looking at the data more closely, we identified certain critical collocations. The exact procedure can be found in the annex.

We then listed the identified collocations and saves this list as the value frame "collocations":

```{r list identified collocations}
collocations <- c(
  
  # climate collocations
  "climate change sceptic*", "climate change agreement*", "emission* trad*", "carbon tax", "carbon target*", "pric* carbon", "net zero carbon", "zero carbon", "low carbon", "carbon reduction", "carbon footprint", "emission* standard*", "carbon dioxide emission*", "emission* target*","emission* reduction*", "tackle emission*", "global emission*", "greenhouse gas emission*", 
  
  # name collocations
  "co emission*", "Prime minister may*", "prime minister johnson*", "climate talk*", "climate conference", "climate issue*", "climate goal*", "renewable energ*", "green energy", "clean energ*", "energy efficien*", "prime minister cameron*", "Lib Dems", "fall* emission*", "Strike Climate", "Sadiq Khan*", "president obama*", "New Zealand", "saudi arabia", "Intergovernmental Panel Climate Change*", "energy saving", "saving energy", "nuclear energy", "climate protection", "climate resilience",  "united nations*", "general assembly", "green new deal", "greenhouse gas", "paris climate agreement", 
  
  # 
  "paris agreement", "climate agreement*", "green growth", "climate action", "climate risk*", "fossil fuel compan*", "energy compan*", "climate denier*", "climate change*", "recep tayyip erdogan*", "climate negotiation*", "fossil fuel*", "climate finance", "climate impact*", "great barrier reef", "paris agreement*", "european union*", "climate catastrophe",  "prime minister*", "net zero", "carbon emissions", "young people*", "human caused", "climate threat", "energy mix", "energy intensiv*", "energy security", "climate refugee*", "climate movement*", "pre industrial", "climate crisis", "climate breakdown", "carbon capture storage", "carbon capture", "global warming", "green industrial revolution", "greta thunberg*", "climate fund", "new york", "david attenborough*", "carbon dioxide", "extreme weather events", "supreme court", "Theresa May*", "David Cameron*", "Jeremy Corbyn*", "Bank England", "House Commons", "Donald Trump*", "Boris Johnson*", "Philip Hammond", "Ed Miliband", "Tom Watson", "Extinction Rebellion*", "North Sea", "Downing Street", "Washington Post", "Fridays Future", "Climate Strike*", "Kim Darroch*", "Conservative Party*", "Dominic Grieve*", "Tony Blair*", "Steve Barclay*", "South Africa*", "Labour Party*", "labour government", "Scott Morrison*", "Climate Action", "San Francisco", "BBC Radio", "British Petroleum", "pride london", "Jeremy Hunt", "Northern Ireland", "Mrs Thatcher", "John McDonnell", "president trump", "Liberal Democrats", "Friends Earth", "White House", "El Niño", "United Kingdom*", "Miami Beach", "United States*", "World Bank", "Mrs May*", "Mr Johnson*", "George Osborne*", "Mr Trump*", "climate emergency", "climate justice", "climate activist*", "climate science", "climate scientist*", "climate summit", "climate protest*", "climate polic*", "climate sceptic", "climate model*", "climate deal", "climate chaos", "climate solutions", "cut* emission*", "reduc* emission*", "zero emission*", "ecological emergency", "climate sceptic*")
```

Then we compounded the identified collocations in the token list:

```{r compound collocations}
toks_comp <- tokens_compound(toks, pattern = phrase(collocations), case_insensitive = TRUE)
```

# Most Frequent Tokens

For the PCA, in line with Greussing and Boomgaarden (2017), we identified the 500 most frequent tokens.
However, because this is a training set, we have not yet fully treated them. This explains the bug-tokens such as "get" or "pic.twitter.com". That step will be fully executed after having further explored the bugs within the individual ".txt" documents.

```{r frequent tokens}
tok_dfm <- dfm(toks_comp, tolower = TRUE)
tok_dfm <- dfm_trim(tok_dfm, tolower = FALSE, min_docfreq = 10, docfreq_type = 
                      c("count"))
textstat <- textstat_frequency(tok_dfm)
textstat
```

To identify the most frequent terms, we order tokens using the function "textstat_frequency"

# Weighted Frequency 

For the final analysis, we will select only the relevant tokens (as in, those which remain after rigorous cleaning) and apply the tf-idf function to calculate their "importance" score.

```{r weighted frequency}
tok_tfidf <- dfm_tfidf(tok_dfm)
textstat_weight <- textstat_frequency(tok_tfidf, force = TRUE)
textstat_weight
```

We wrote a csv at this point to have a closer look at the data:

```{r csv check, message = FALSE}
write.csv(textstat_weight, file = "textstat_weight.csv")
```

# Stemming

As a comparison, we applied stemming. Stemming reduces the tokens to their word-stems.

```{r frequent tokens stemmed}
tok_stem_dfm <- dfm(toks_comp, stem = TRUE, tolower = TRUE)
tok_stem_dfm <- dfm_trim(tok_stem_dfm, min_docfreq = 10, docfreq_type = c("count"))
textstat_stem <- textstat_frequency(tok_stem_dfm)
textstat_stem
```

For the final analysis, we will select only the relevant tokens (as in, those which remain after rigorous cleaning) and apply the tf-idf function to calculate their "importance" score.

```{r weighted frequency stemmed}
tok_stem_tfidf <- dfm_tfidf(tok_stem_dfm)
textstat_stem_weight <- textstat_frequency(tok_stem_tfidf, force = TRUE)
textstat_stem_weight
```

# Principal Component Analysis (PCA)

We then applied the principal() function from the "psych" package to test for the presence of latent variables. In doing so, we defined the number of frames as 5 (RCA1 to RC5). The most logical number of frames is still to be determined, this depends on the variation the different frames are able to explain. We also tested the prcomp function, but found its suitability for the analysis of categorical variables to be limited compared to principal(), which is designed for categorical variables specifically. 

```{r list final selection}
final_select <- c("fossil_fuel_compan*", "climate_breakdown", "green_growth", "united_nations", "	fossil_fuel", "renewable_energy", "net_zero", "new_york", "climate_emergency", "extinction_rebellion*", "climate_strike", "greta_thunberg", "united_kingdom", "low_carbon", "clean energy", "climate_scienc*", "young_people", "climate_scientists", "energy_efficiency", "zero_carbon", "climate_summit", "climate_change_agreement", "green_new_deal", "intergovernmental_panel_climate_change", "scien**", "energy_companies", "human_caused", "climate_sceptic*", "climate_deal", "general_assembly", "el_niño", "impact", "carbon_capture_storage", "climate_talk*", "climate_deni*", "price_carbon", "climate_negotiations", "paris_climate_agreement", "climate_risk", "climate_impact*")
```

```{r collocations final selection}
collocations_gsub <- gsub(" ", "_", collocations, ignore.case = T)
```

For the final selection, we used a list of collocations. At this point, we could either use the "final_select" or the "collocations_gsub" list of tokens, depending on what which list we entered in "phrase()".

!!! fill in "phrase()" accordingly !!!

!! check for padding !!

```{r final selection}
toks_anal <- tokens_select(toks_comp, pattern = phrase(final_select), 
                           selection = c("keep"), case_insensitive = TRUE, padding = FALSE, window = 0, verbose = TRUE)
```

Then, we transformed the token list for the final selection into a data frame and checked for the individual token frequency::

```{r final selection frequent tokens}
tok_anal_dfm <- dfm(toks_anal, tolower = TRUE)
tok_anal_dfm <- dfm_trim(tok_anal_dfm, tolower = FALSE, min_docfreq = 1, 
                         docfreq_type = c("count"))
textstat_anal <- textstat_frequency(tok_anal_dfm)
textstat_anal
```

Again, we applied the tf-idf score:

```{r weighted final selection}
tok_anal_tfidf <- dfm_tfidf(tok_anal_dfm)
textstat_weight_anal <- textstat_frequency(tok_anal_tfidf, force = TRUE)
textstat_weight_anal
```

And then applied the principal() function to see identify frames ("nfactors" defines the number of expected frames):

```{r PCA final selection}
PCA <- principal(as.matrix(tok_anal_tfidf), nfactors=8, rotate="varimax")
PCA
```

We exported the results:

```{r}
write.csv(PCA, file = "PCA.csv")
```

We also did a PCA with all tokens:

```{r PCA all tokens}
principal(as.matrix(tok_tfidf), nfactors=5, rotate="varimax")

```

We also applied the principal() function to all stemmed tokens.

```{r PCA all tokens stemmed}

principal(as.matrix(tok_stem_tfidf), nfactors=5, rotate="varimax")

```

Overall, we found our analysis to be helpful in identify frames. However, our ability to extrapolate meaningful results was hindered by the poor quality of data and the need for further refinement thereof. We intend to address these issues ahead of the execution of the final analysis.

# Annex

In the annex, we collected the code used in the process that is not essential to the final calculation.

# Identification of Context

To check the quality of our data, we investigated the context of "suspicious" tokens:

```{r kwic "document", echo = FALSE}
kw_doc <- kwic(toks_comp, pattern =  'Document', ignore.case = FALSE)
head(kw_doc, 10)
write.csv(kw_doc, file = "kw_doc.csv")
```

```{r kwic "limited", echo = FALSE}
kw_limited <- kwic(toks_comp, pattern =  'limited')
head(kw_limited, 10)
write.csv(kw_limited, file = "kw_limited.csv")
```

```{r kwic "www", echo = FALSE}
kw_www <- kwic(toks_comp, pattern =  'www')
head(kw_www, 10)
write.csv(kw_www, file = "kw_www.csv")
```

```{r kwic "status", echo = FALSE}
kw_status <- kwic(toks_comp, pattern =  'status')
head(kw_status, 10)
write.csv(kw_status, file = "kw_status.csv")
```

```{r kwic "kingdom", echo = FALSE}
kw_kingdom <- kwic(toks_comp, pattern =  'Ireland')
head(kw_kingdom, 10)
```

```{r kwic "climate", echo = FALSE}
kw_climate <- kwic(toks_comp, pattern =  'climate')
head(kw_climate, 100)
```

```{r kwic "green", echo = FALSE}
kw_green <- kwic(toks_comp, pattern =  'green')
head(kw_green, 100)
```

# Identification of Collocations

We looked for collocations with four words:

```{r identify collocations(4), echo = FALSE}
toks_4 <- textstat_collocations(toks, size = 4, min_count = 5, tolower = TRUE)
head(toks_4, 100)
```

We looked for collocations with three words:

```{r identify collocations(3), echo = FALSE}
toks_3 <- textstat_collocations(toks, size = 3, min_count = 10, tolower = TRUE)
head(toks_3, 100)
```

We looked for collocations with two words:

```{r identify collocations(2), echo = FALSE}
toks_2 <- textstat_collocations(toks, size = 2, min_count = 10, tolower = TRUE)
head(toks_2, 100)
```

By looking at capital letters, we could identify names as collocations:

```{r identify names, echo = FALSE}
toks_cap <- tokens_select(toks_comp, pattern = '^[A-Z]',
                               valuetype = 'regex',
                               case_insensitive = FALSE, 
                               padding = TRUE)
toks_cap <- textstat_collocations(toks_cap, min_count = 15, tolower = FALSE)
head(toks_cap, 100)
```

As the term "climate" appeared in many different contexts, we checked for "climate"-collocations seperately (and did the same with other terms):

```{r identify "climate" collocations, echo = FALSE}
toks_climate <- tokens_select(toks_comp, pattern = "climate", selection =                       c("keep"), case_insensitive = TRUE, padding = TRUE, window = 1)
toks_climate <- textstat_collocations(toks_climate, size = 2, min_count = 10,                    tolower = FALSE)
head(toks_climate, 100)
```

```{r identify "people" collocations, echo = FALSE}
toks_people <- tokens_select(toks_comp, pattern = "people", selection = c("keep"),             case_insensitive = TRUE, padding = TRUE, window = 1)
toks_people <- textstat_collocations(toks_people, min_count = 10, tolower = FALSE)
head(toks_people, 100)
```

```{r identify "government" collocations, echo = FALSE}
toks_govern <- tokens_select(toks_comp, pattern = "government", selection =                   c("keep"), case_insensitive = TRUE, padding = TRUE, window = 1)
toks_govern <- textstat_collocations(toks_govern, min_count = 10, tolower = FALSE)
head(toks_govern, 100)
```

```{r identify "energy" collocations, echo = FALSE}
toks_energy <- tokens_select(toks_comp, pattern = "energy*", selection =                   c("keep"), case_insensitive = TRUE, padding = TRUE, window = 1)
toks_energy <- textstat_collocations(toks_energy, min_count = 8, tolower =                FALSE)
head(toks_energy, 100)
```

```{r identify "emissions" collocations, echo = FALSE}
toks_emission <- tokens_select(toks_comp, pattern = "emission*", selection =                   c("keep"), case_insensitive = TRUE, padding = TRUE, window = 1)
toks_emission <- textstat_collocations(toks_emission, min_count = 3, tolower =                FALSE)
head(toks_emission, 100)
```

```{r identify "climate change" collocations, echo = FALSE}
toks_CC <- tokens_select(toks_comp, pattern = "climate_change", selection =                   c("keep"), case_insensitive = TRUE, padding = TRUE, window = 1)
toks_CC <- textstat_collocations(toks_CC, min_count = 8, tolower =                            FALSE)
head(toks_CC, 100)
```

```{r identify "carbon" collocations, echo = FALSE}
toks_carbon <- tokens_select(toks_comp, pattern = "carbon", selection =                   c("keep"), case_insensitive = TRUE, padding = TRUE, window = 1)
toks_carbon <- textstat_collocations(toks_carbon, min_count = 8, tolower =                            FALSE)
head(toks_carbon, 100)
```

```{r identify "environment" collocations, echo = FALSE}
toks_environ <- tokens_select(toks_comp, pattern = "environment*", selection =                   c("keep"), case_insensitive = TRUE, padding = TRUE, window = 1)
toks_environ <- textstat_collocations(toks_environ, min_count = 8, tolower =                            FALSE)
head(toks_environ, 100)
```

```{r identify "environment" collocations, echo = FALSE}
toks_green <- tokens_select(toks_comp, pattern = "green", selection =                   c("keep"), case_insensitive = TRUE, padding = TRUE, window = 1)
toks_green <- textstat_collocations(toks_green, min_count = 8, tolower =                            FALSE)
head(toks_green, 100)
```
