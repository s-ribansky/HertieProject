---
title: "Midterm_report"
author: "SR"
date: "20/10/2019"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

#Packages used for this analysis

```{r cars}

setwd("/Users/...")
library(tidyverse)
library(psych)
library(ggplot2)
library(devtools)
library(readtext)
library(quanteda)
library(quanteda.corpora)
library(tm)
library(SnowballC)
library(dplyr)
library(tidyr)
```

 
 read documents from wording directory
 
```{r pressure, echo=FALSE}

textx <- readtext("701-800/*",
                  docvarsfrom = "filenames",
                  dvsep="-")
```

Because every .txt file contained the document variables in rows and not in columns or nodes (as opposed to a
regularly structured document), we pre-processed the data to enable the proper labelling and subsequent treatment
thereof.

```{r}
txtx_split <- str_split(textx$text, "\n", simplify = TRUE)
txtx_split_merged <- as.data.frame(txtx_split) %>% unite(text, c(V10:ncol(txtx_split)), sep = " ", remove = FALSE)
txtx_split_merged[11:ncol(txtx_split_merged)] <- list(NULL)
colnames(txtx_split_merged) <- c("title", "author", "length", "date", "time", "medium", "source", "language", "source2", "text")
txtx_split_merged2 <- separate(txtx_split_merged, c(date), c("day","month","year"), sep = " ", remove = TRUE)
txtx_split_merged2 <- separate(txtx_split_merged2, c(length), c("wordcount","words"), sep = " ", remove = TRUE)
txtx_split_merged2$words <- NULL
txtx_split_merged2 <- unite(txtx_split_merged2, "id", c(year, medium, title), sep = "_", remove = FALSE)

```

Amid the low quality and low degree of harmonisation of the data, we proceeded to further manually process the data to have a usable dfm. 

```{r}
txtx_split_merged3 <- txtx_split_merged2
txtx_split_merged3$text <- gsub("\\d", "", txtx_split_merged3$text)
txtx_split_merged3$text <- gsub("[(c)]", "", txtx_split_merged3$text, ignore.case = F)
txtx_split_merged3$text <- gsub("\\d", " ", txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("Times Newspapers Limited", "", txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("Copyright", "", txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("All rights reserved", "", txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("The Guardian", " ", txtx_split_merged3$text, ignore.case = T)
txtx_split_merged3$text <- gsub("Independent Print Ltd", "", txtx_split_merged3$text, ignore.case = T)

```

Next, we created a data corpus

```{r}
txt_corpus <- corpus(txtx_split_merged3, docid_field = "id", text_field = "text")
txt_corpus_dfm <- dfm(txt_corpus)


```

Then we created token vars that we would use for the first part of our analysis, the principal component analysis
(PCA). For this purpose, in line with Greussing and Boomgaarden (2017), we identified 500 most frequent tokens.
However, because this is a training set, we have not yet fully treated them. That step will be fully executed after
having further explored the bugs within the individual .txt documents. 

```{r}
toks <- tokens(txt_corpus, what="word", remove_numbers=TRUE, remove_punct=TRUE, remove_hyphens=TRUE,
               include_docvars = TRUE)

tok_dfm <- dfm(toks, tolower = TRUE, stem = TRUE ,  remove = stopwords("en"))
test_tok_dfm <- tok_dfm
test_tok_dfm1 <- dfm_trim(test_tok_dfm, min_termfreq = 500, termfreq_type = "rank")
test_tok_dfm2 <- test_tok_dfm1
head(featnames(test_tok_dfm2), 50)
test_tok_dfm3 <- dfm_tfidf(test_tok_dfm1)
View(test_tok_dfm3)
head(featnames(test_tok_dfm3), 50)



```

We then applied the principal() function from the "psych" package to test for the presence of latent variables. We
also tested the prcomp function, but found its suitability for the analysis of categorical variables to be limited
compared to principal(), which is designed for categorical variables specifically. 

```{r}

principal(as.matrix(test_tok_dfm3), nfactors=5, rotate="varimax")

```

Overall, we found our analysis to be helpful in helping us identify frames. However, our ability to extrapolate
meanigful results was hindered by the poor quality of data and the need for further refinement thereof. Most
obviously, we still omitted severalinstances of generic copyright phrases, which resulted in the tokens generated by
these phrases to be the determining factors of our phrames. We intend to address these issues ahead of the execution
of the final analysis. 



